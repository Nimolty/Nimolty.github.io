<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Predictive Inverse Dynamics Models are Scalable Learners for Robotic Manipulation">
  <meta name="keywords" content="Robotic Manipulation, Language Models">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Seer</title>

  <script async src="https://www.googletagmanager.com/gtag/js?id=G-FV4ZJ9PVSV"></script>  
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-FV4ZJ9PVSV');
  </script>


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body onload="updateInteractive();">
<body onload="updatecrossInteractive();"></body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-fullhd">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Predictive Inverse Dynamics Models <br> are Scalable Learners for Robotic Manipulation</h1>
          <h3 class="title is-4 conference-authors"><a target="_blank" >In Submission</a></h3>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a target="_blank" href="https://github.com/Nimolty?tab=overview&from=2024-01-01&to=2024-01-31">Yang Tian</a><sup>*,1,2,3,4</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://yangsizhe.github.io/">Sizhe Yang</a><sup>*,5,1</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://scholar.google.com/citations?user=kYrUfMoAAAAJ&hl=zh-CN">Jia Zeng</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" >Ping Wang</a><sup>4</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="http://dahua.site/">Dahua Lin</a><sup>5</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://zsdonghao.github.io/">Hao Dong</a><sup>2,3</sup>
            </span>
            <span class="author-block">
              <a target="_blank" href="https://oceanpang.github.io/">Jiangmiao Pang</a><sup>&#x2709;1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Shanghai AI Laboratory,</span>
            <span class="author-block"><sup>2</sup>CFCS, School of CS, Peking University, Peking University, Beijing, China,</span>
            <span class="author-block"><sup>3</sup>National Key Laboratory for Multimedia Information Processing, Peking University, Beijing, China,</span>
            <span class="author-block"><sup>4</sup>National Engineering Research Center for Software Engineering, Peking University, Beijing, China,</span>
            <span class="author-block"><sup>5</sup>Chinese University of Hong Kong,</span>
            <span class="author-block"><sup>*</sup>Equal Contribution, Author ordering determined by coin flip,</span>
            <span class="author-block"><sup>&#x2709;</sup>Corresponding Author</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a target="_blank" href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

            <!-- Arxiv Link. -->
            <span class="link-block">
              <a target="_blank" href=""
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fas fa-file"></i>
                </span>
                <span>ArXiv</span>
              </a>
            </span>

            <!-- Video Link. -->
            <span class="link-block">
              <a target="_blank" href=""
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fab fa-youtube"></i>
                </span>
                <span>Video</span>
              </a>
            </span>

            <!-- Code Link. -->
            <span class="link-block">
              <a target="_blank" href=""
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fab fa-github"></i>
                </span>
                <span>Code</span>
                </a>
            </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Current efforts to learn scalable policies in robotic manipulation primarily fall into two categories: one focuses on "action," which involves behavior cloning from extensive collections of robotic data, while the other emphasizes "vision," enhancing model generalization by pre-training representations or generative models, also referred to as world models, using large-scale visual datasets.
            This paper presents an end-to-end paradigm that predicts actions using inverse dynamics models conditioned on the robot's forecasted visual states, named Predictive Inverse Dynamics Models (PIDM).
            By closing the loop between vision and action, the end-to-end PIDM can be a better scalable action learner.
            In practice, we use Transformers to process both visual states and actions, naming the model <b>Seer</b>. 
            It is initially pre-trained on large-scale robotic datasets, such as DROID, and can be adapted to real-world scenarios with a little fine-tuning data.
            Thanks to large-scale, end-to-end training and the continuous synergy between vision and action at each execution step, <b>Seer</b> significantly outperforms state-of-the-art methods across both simulation and real-world experiments. 
            It achieves improvements of 13% on the LIBERO-LONG benchmark, 22% on CALVIN ABC-D, and 43% in real-world tasks.
            Notably, it demonstrates superior generalization for novel objects, lighting conditions, and environments under high-intensity disturbances.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>
</section>


<section class="section">
  <div class="container is-max-widescreen">

    <div class="rows">


    <!-- Animation. -->
    <div class="rows is-centered has-text-centered ">
      <div class="row is-full-width">
        <h2 class="title is-3">Pipeline of Seer</h2>

        <!-- Interpolating. -->
        <div class="content has-text-justified">
        <!-- <br> -->
        </div>
        <img src="media/figures/seer_method.jpg" class="interpolation-image" />
        </br>
        </br>
          <p class="content has-text-justified">
            <b>Seer</b> consists of three  parts: Multi-Modal Encoder, Conditional Visual Foresight and Inverse Dynamics Prediction.
            In Multi-Modal Encoder, <b>Seer</b> incorporates the foresight token <b>[FRS]</b> and the action token <b>[INV]</b>. 
            Both tokens attend to the RGB images, language tokens, and robot state tokens, with <b>[INV]</b> also attending to <b>[FRS]</b>.
            In Conditional Visual Foresight, the encoded <b>[FRS]</b>, along with new mask tokens, aims to reconstruct future RGB images. 
            In Inverse Dynamics Prediction, the encoded <b>[INV]</b> and other tokens speculate intermediary actions.
          </p>

    </div>
  </div>
</section>








<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-centered">
          <p>
            Website template borrowed from <a href="https://github.com/voxposer/voxposer.github.io">VoxPoser</a>. 
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>


</body>
</html>
