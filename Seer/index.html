<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Predictive Inverse Dynamics Models are Scalable Learners for Robotic Manipulation">
  <meta name="keywords" content="Robotic Manipulation, Language Models">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Seer</title>

  <script async src="https://www.googletagmanager.com/gtag/js?id=G-FV4ZJ9PVSV"></script>  
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-FV4ZJ9PVSV');
  </script>


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body onload="updateInteractive();">
<body onload="updatecrossInteractive();"></body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-fullhd">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">RoboKeyGen: Robot Pose and Joint Angles Estimation via <br> Diffusion-based 3D Keypoint Generation</h1>
          <h3 class="title is-4 conference-authors"><a target="_blank" href="https://2024.ieee-icra.org/">ICRA 2024</a></h3>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a target="_blank" href="https://github.com/Nimolty?tab=overview&from=2024-01-01&to=2024-01-31">Yang Tian</a><sup>*,1,2</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://jiyao06.github.io/">jiyao Zhang</a><sup>*,1,2</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" >Guowei Huang</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" >Bin Wang</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" >Ping Wang</a><sup>4</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://oceanpang.github.io/">Jiangmiao Pang</a><sup>5</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://zsdonghao.github.io/">Hao Dong</a><sup>&#x2709;,1,2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>CFCS, School of CS, Peking University,</span>
            <span class="author-block"><sup>2</sup>National Key Laboratory for Multimedia Information Processing, </span>
            <span class="author-block"><sup>3</sup>Huawei,</span>
            <span class="author-block"><sup>4</sup>School of Software & Microelectronics and National Engineering Research Center for Software Engineering, Peking University,</span>
            <span class="author-block"><sup>5</sup>Chinese University of Hong Kong,</span>
            <span class="author-block"><sup>*</sup>Equal Contribution,</span>
            <span class="author-block"><sup>&#x2709;</sup>Corresponding Author</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a target="_blank" href="robokeygen.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

            <!-- Arxiv Link. -->
            <span class="link-block">
              <a target="_blank" href=""
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fas fa-file"></i>
                </span>
                <span>ArXiv</span>
              </a>
            </span>

            <!-- Video Link. -->
            <span class="link-block">
              <a target="_blank" href="https://www.youtube.com/watch?v=oD1pSinGJqM"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fab fa-youtube"></i>
                </span>
                <span>Video</span>
              </a>
            </span>

            <!-- Code Link. -->
            <span class="link-block">
              <a target="_blank" href="https://github.com/Nimolty/RoboKeyGen/tree/main"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fab fa-github"></i>
                </span>
                <span>Code</span>
                </a>
            </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="teaser">
  <div class="teaser flag">

    <div class="rows">
      <h2 class="title is-3"></h2>
      <div style="text-align: center;">
        <img src="media/figures/Teaser.jpg" class="interpolation-image" style="width: 20%; height: 20%;"/>
      </div>
      <br>
      <h2 class="subtitle has-text-centered">
        Given <b>RGB</b> images, we aim to estimate the robot pose and joint angles.<br> 
        We achieve this goal by decoupling it into two more tractable tasks: <b>2D keypoints detection</b> 
        and <b>lifting 2D keypoints to 3D</b>.
      </h2>
    </div>

  </div>

</section>




<h2 class="subtitle has-text-centered">
</br>
  With the robot pose and joint angles predicted by <b>RoboKeyGen</b>, robots could implement downstream tasks such as <b>Visual Servoing</b> 
  and <b>Grasping</b>.
</h2>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Estimating robot pose and joint angles is significant in robotics, 
            enabling applications like robot collaboration and online hand-eye calibration.
            However, the introduction of unknown joint angles makes prediction more complex than simple robot pose estimation, 
            due to its higher dimensionality.
            Previous methods either regress 3D keypoints directly or utilise a render&compare strategy. 
            These approaches often falter in terms of performance or efficiency and grapple with the cross-camera gap problem.
            This paper presents a novel framework that bifurcates the high-dimensional prediction task into two manageable subtasks: 
            2D keypoints detection and lifting 2D keypoints to 3D. 
            This separation promises enhanced performance without sacrificing the efficiency innate to keypoint-based techniques.
            A vital component of our method is the lifting of 2D keypoints to 3D keypoints. 
            Common deterministic regression methods may falter when faced with uncertainties from 2D detection errors or self-occlusions.
            Leveraging the robust modeling potential of diffusion models, we reframe this issue as conditional 3D keypoints generation.
            To bolster cross-camera adaptability, we introduce the <i>Normalised Camera Coordinate Space (NCCS)</i>, 
            ensuring alignment of estimated 2D keypoints across varying camera intrinsics.
            Experimental results demonstrate that the proposed method outperforms the state-of-the-art render&compare method and achieves higher inference speed.
            Furthermore, the tests accentuate our method's robust cross-camera generalisation capabilities.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>


  <!-- Paper video. -->
  <br>
  <br>
  <br>

  <div class="container is-max-widescreen">
    <div class="rows is-centered has-text-centered">
      <div class="rows">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/oD1pSinGJqM?si=sdVF7U72z1rg8cHO"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
  </div>

</section>




<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-centered">
          <p>
            Website template borrowed from <a href="https://github.com/voxposer/voxposer.github.io">VoxPoser</a>. 
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>


</body>
</html>
